MODEL GENERATOR: PyTorch: MLP(
  (mlp): Sequential(
    (0): Linear(in_features=25, out_features=128, bias=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=128, out_features=128, bias=True)
    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.2, inplace=False)
    (8): Linear(in_features=128, out_features=64, bias=True)
    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU()
    (11): Dropout(p=0.2, inplace=False)
    (12): Linear(in_features=64, out_features=1, bias=True)
  )
)
###################-STARTED TRAINING-###################
TRAIN:	 Epoch:   1 | BCE Loss: 0.46317 
TRAIN:	 Epoch:   2 | BCE Loss: 0.45382 
TRAIN:	 Epoch:   3 | BCE Loss: 0.45218 
TRAIN:	 Epoch:   4 | BCE Loss: 0.45124 
TRAIN:	 Epoch:   5 | BCE Loss: 0.45050 
TRAIN:	 Epoch:   6 | BCE Loss: 0.44976 
TRAIN:	 Epoch:   7 | BCE Loss: 0.44892 
TRAIN:	 Epoch:   8 | BCE Loss: 0.44829 
TRAIN:	 Epoch:   9 | BCE Loss: 0.44771 
TRAIN:	 Epoch:  10 | BCE Loss: 0.44692 
TRAIN:	 Epoch:  11 | BCE Loss: 0.44617 
TRAIN:	 Epoch:  12 | BCE Loss: 0.44566 
TRAIN:	 Epoch:  13 | BCE Loss: 0.44512 
TRAIN:	 Epoch:  14 | BCE Loss: 0.44458 
TRAIN:	 Epoch:  15 | BCE Loss: 0.44394 
TRAIN:	 Epoch:  16 | BCE Loss: 0.44338 
TRAIN:	 Epoch:  17 | BCE Loss: 0.44260 
TRAIN:	 Epoch:  18 | BCE Loss: 0.44128 
TRAIN:	 Epoch:  19 | BCE Loss: 0.44019 
TRAIN:	 Epoch:  20 | BCE Loss: 0.43921 
TRAIN:	 Epoch:  21 | BCE Loss: 0.43805 
TRAIN:	 Epoch:  22 | BCE Loss: 0.43747 
TRAIN:	 Epoch:  23 | BCE Loss: 0.43680 
TRAIN:	 Epoch:  24 | BCE Loss: 0.43621 
TRAIN:	 Epoch:  25 | BCE Loss: 0.43581 
TRAIN:	 Epoch:  26 | BCE Loss: 0.43528 
TRAIN:	 Epoch:  27 | BCE Loss: 0.43488 
TRAIN:	 Epoch:  28 | BCE Loss: 0.43447 
TRAIN:	 Epoch:  29 | BCE Loss: 0.43404 
TRAIN:	 Epoch:  30 | BCE Loss: 0.43372 
TRAIN:	 Epoch:  31 | BCE Loss: 0.43337 
TRAIN:	 Epoch:  32 | BCE Loss: 0.43280 
TRAIN:	 Epoch:  33 | BCE Loss: 0.43294 
TRAIN:	 Epoch:  34 | BCE Loss: 0.43251 
TRAIN:	 Epoch:  35 | BCE Loss: 0.43264 
TRAIN:	 Epoch:  36 | BCE Loss: 0.43211 
TRAIN:	 Epoch:  37 | BCE Loss: 0.43212 
TRAIN:	 Epoch:  38 | BCE Loss: 0.43211 
TRAIN:	 Epoch:  39 | BCE Loss: 0.43192 
TRAIN:	 Epoch:  40 | BCE Loss: 0.43160 
TRAIN:	 Epoch:  41 | BCE Loss: 0.43147 
TRAIN:	 Epoch:  42 | BCE Loss: 0.43134 
TRAIN:	 Epoch:  43 | BCE Loss: 0.43105 
TRAIN:	 Epoch:  44 | BCE Loss: 0.43104 
TRAIN:	 Epoch:  45 | BCE Loss: 0.43089 
TRAIN:	 Epoch:  46 | BCE Loss: 0.43095 
TRAIN:	 Epoch:  47 | BCE Loss: 0.43072 
TRAIN:	 Epoch:  48 | BCE Loss: 0.43044 
TRAIN:	 Epoch:  49 | BCE Loss: 0.43034 
TRAIN:	 Epoch:  50 | BCE Loss: 0.43044 
TRAIN:	 Epoch:  51 | BCE Loss: 0.43048 
TRAIN:	 Epoch:  52 | BCE Loss: 0.43003 
TRAIN:	 Epoch:  53 | BCE Loss: 0.43014 
TRAIN:	 Epoch:  54 | BCE Loss: 0.43014 
TRAIN:	 Epoch:  55 | BCE Loss: 0.42993 
TRAIN:	 Epoch:  56 | BCE Loss: 0.42967 
TRAIN:	 Epoch:  57 | BCE Loss: 0.42974 
TRAIN:	 Epoch:  58 | BCE Loss: 0.42949 
TRAIN:	 Epoch:  59 | BCE Loss: 0.42953 
TRAIN:	 Epoch:  60 | BCE Loss: 0.42952 
TRAIN:	 Epoch:  61 | BCE Loss: 0.42938 
TRAIN:	 Epoch:  62 | BCE Loss: 0.42940 
TRAIN:	 Epoch:  63 | BCE Loss: 0.42897 
TRAIN:	 Epoch:  64 | BCE Loss: 0.42914 
TRAIN:	 Epoch:  65 | BCE Loss: 0.42892 
TRAIN:	 Epoch:  66 | BCE Loss: 0.42915 
TRAIN:	 Epoch:  67 | BCE Loss: 0.42901 
TRAIN:	 Epoch:  68 | BCE Loss: 0.42879 
TRAIN:	 Epoch:  69 | BCE Loss: 0.42888 
TRAIN:	 Epoch:  70 | BCE Loss: 0.42872 
TRAIN:	 Epoch:  71 | BCE Loss: 0.42847 
TRAIN:	 Epoch:  72 | BCE Loss: 0.42851 
TRAIN:	 Epoch:  73 | BCE Loss: 0.42855 
TRAIN:	 Epoch:  74 | BCE Loss: 0.42839 
TRAIN:	 Epoch:  75 | BCE Loss: 0.42845 
TRAIN:	 Epoch:  76 | BCE Loss: 0.42853 
TRAIN:	 Epoch:  77 | BCE Loss: 0.42820 
TRAIN:	 Epoch:  78 | BCE Loss: 0.42812 
TRAIN:	 Epoch:  79 | BCE Loss: 0.42806 
TRAIN:	 Epoch:  80 | BCE Loss: 0.42787 
TRAIN:	 Epoch:  81 | BCE Loss: 0.42815 
TRAIN:	 Epoch:  82 | BCE Loss: 0.42780 
TRAIN:	 Epoch:  83 | BCE Loss: 0.42804 
TRAIN:	 Epoch:  84 | BCE Loss: 0.42775 
TRAIN:	 Epoch:  85 | BCE Loss: 0.42813 
TRAIN:	 Epoch:  86 | BCE Loss: 0.42796 
TRAIN:	 Epoch:  87 | BCE Loss: 0.42772 
TRAIN:	 Epoch:  88 | BCE Loss: 0.42772 
TRAIN:	 Epoch:  89 | BCE Loss: 0.42746 
TRAIN:	 Epoch:  90 | BCE Loss: 0.42732 
TRAIN:	 Epoch:  91 | BCE Loss: 0.42761 
TRAIN:	 Epoch:  92 | BCE Loss: 0.42741 
TRAIN:	 Epoch:  93 | BCE Loss: 0.42746 
TRAIN:	 Epoch:  94 | BCE Loss: 0.42755 
TRAIN:	 Epoch:  95 | BCE Loss: 0.42733 
TRAIN:	 Epoch:  96 | BCE Loss: 0.42719 
TRAIN:	 Epoch:  97 | BCE Loss: 0.42717 
TRAIN:	 Epoch:  98 | BCE Loss: 0.42712 
TRAIN:	 Epoch:  99 | BCE Loss: 0.42679 
TRAIN:	 Epoch: 100 | BCE Loss: 0.42715 
###################-FINISHED TRAINING-###################
###################-STARTED TESTING-###################
TEST:	 BCE Loss: 0.46422 | ROC AUC: 0.51622 | Accuracy: 0.82866
###################-FINISHED TESTING-###################
Execution Time: 41m 23.3sec ==> hour feature parsed into ["day", "weekday", "hour_of_day"], label_encoding rest, no embeddings
